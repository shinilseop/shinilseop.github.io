---
title:  "BentoML Inference Server (ê°„ë‹¨í•œ ì¸í¼ëŸ°ìŠ¤ ì„œë²„ with Docker)"
excerpt: "BentoML Inference Server Example"

categories:
  - MLOps
tags:
  - [MLOps, BentoML, Docker, Inference Server]

toc: true
toc_sticky: true
 
date: 2022-06-18
last_modified_at: 2022-06-18

---

# BentoML

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled.png)

BentoML simplifies ML model deployment and serves your models at production scale.

## **Feature Highlights**

âœ¨Â Model Serving the way you need it

- **Online serving**Â via REST API or gRPC
- **Offline scoring**Â on batch datasets with Apache Spark, or Dask.
- **Stream serving**Â with Kafka, Beam, and Flink

ğŸ±Â Easily go from training to model serving in production

- All common ML libraries natively supported! - Tensorflow, PyTorch, XGBoost, Scikit-Learn and many more
- StandardÂ `.bento`Â format for packaging code, models and dependencies for easy versioning and deployment
- Automatically setup Dockerfile, CUDA and python packages for model serving
- Designed to work with any training pipeline or experimentation platform

ğŸÂ Python-first, scales with powerful optimizations

- Scale model inference workers separately from business logic and feature processing code
- Adaptive batching dynamically groups inference requests for optimal performance
- Inference graphs with multiple models automatically orchestrated with Yatai on Kubernetes

ğŸš¢Â Deployment workflow made for production

- ğŸ³Â Automatically generate docker images for production deployment
- [ğŸ¦„ï¸Â Yatai](https://github.com/bentoml/yatai): Model Deployment at scale on Kubernetes
- [ğŸš€Â bentoctl](https://github.com/bentoml/bentoctl): Fast model deployment on any cloud platform

ìœ„ì˜ ë‚´ìš©ì€ BentoML ê³µì‹ ê¹ƒí—ˆë¸Œì—ì„œ ì œê³µí•˜ê³  ìˆëŠ” ë‚´ìš©ì´ë‹¤.

BentoMLì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ê³  ë‚œ í›„ ì„œë¹™ì„ í•˜ëŠ” ê³¼ì •ì— ìˆì–´ì„œ í•„ìš”í•œ ê¸°ëŠ¥ë“¤ì„ ê°„ë‹¨í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í”„ë¡œë•ì…˜ ë ˆë²¨ì—ì„œ ì œê³µí•˜ëŠ” í”„ë ˆì„ì›Œí¬ì´ë‹¤.

í›ˆë ¨ë˜ì–´ìˆëŠ” ëª¨ë¸ì„ ê°€ì§€ê³  ë„ì»¤ì— ì¸í¼ëŸ°ìŠ¤ ì„œë²„ë¥¼ ë„ì›Œë³´ì

## í™˜ê²½ êµ¬ì„±

### Anaconda miniforge ì„¤ì¹˜

ëª¨ë“ˆ ì„¤ì¹˜ë¥¼ ì§„í–‰í•˜ê¸° ì „ì— íŒŒì´ì¬ í™˜ê²½ì„ ë¶„ë¦¬í•˜ê¸° ìœ„í•´ ì•„ë‚˜ì½˜ë‹¤ë¥¼ ì„¤ì¹˜í•´ì„œ ê°€ìƒí™˜ê²½ì„ êµ¬ì„±í–ˆë‹¤.

[https://github.com/conda-forge/miniforge](https://github.com/conda-forge/miniforge)

ìœ„ì˜ miniforge ê¹ƒ í—ˆë¸Œì— ì ‘ê·¼í•´ì„œ mac os ì¤‘ arm64 ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±ëœê±¸ë¡œ ë‹¤ìš´í•´ì„œ ì„¤ì¹˜í–ˆë‹¤.

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled%201.png)

```bash
# ì„¤ì¹˜
sh ~/Downloads/Miniforge3-MacOSX-arm64.sh

# í™˜ê²½ë³€ìˆ˜ ë„£ê¸°
source ~/miniforge3/bin/activate
```

ì„¤ì¹˜í•˜ê³  ì•„ë‚˜ì½˜ë‹¤ í™˜ê²½ì„ ìƒˆë¡œ í•˜ë‚˜ ìƒì„±í–ˆë‹¤.

```bash
conda create --name bentoml python=3.8

conda activate bentoml
```

### BentoML + í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

```bash
pip install bentoml
pip install transformers # ì‚¬ìš©í• êº¼ë¼ì„œ...
pip install pytorch torchvision torchaudio # ì–˜ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì‚¬ìš©ì˜ˆì •
```

ê¸°ë³¸ì ìœ¼ë¡œ bentoml ì„ ì„¤ì¹˜í•œ í›„ transformersì™€ pytorch ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í–ˆë‹¤.

ì´í›„ ì˜ˆì œ ì½”ë“œë¥¼ ëŒë¦¬ëŠ”ë° bentomlì„ ì„¤ì¹˜í•˜ë©´ì„œ ì¢…ì†ì„±ìœ¼ë¡œ ì„¤ì¹˜ëœ grpc ëª¨ë“ˆì„ ê³„ì† ê°€ì ¸ì˜¤ì§€ ëª»í•˜ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆë‹¤.

êµ¬ê¸€ë§í•˜ë‹¤ë³´ë‹ˆ ì•„ë§ˆ Apple Silicon Mac m1 ì—ì„œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì¸ê±°ê°™ì€ë° í™˜ê²½ì„ ì‹¤í–‰í•œ ê°€ìƒí™˜ê²½ìœ¼ë¡œ ëª»ì¡ê³  ê¸°ë³¸ íŒŒì´ì¬ìœ¼ë¡œ ìê¾¸ ì¡ìœ¼ë ¤ê³ í•˜ëŠ”ê²ƒ ê°™ë‹¤. 

í•´ê²°í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì´ ì œì‹œë˜ì–´ ìˆì—ˆëŠ”ë°

```bash
export GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=1
export GRPC_PYTHON_BUILD_SYSTEM_ZLIB=1
```

ìœ„ì™€ ê°™ì´ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê³  ë‹¤ì‹œ grpcë¥¼ ì„¤ì¹˜í•˜ë¼ëŠ” ë‚´ìš©ì´ì–´ì„œ ë”°ë¼ ì§„í–‰í–ˆì§€ë§Œ í•´ê²°ì€ ì•ˆë¬ë‹¤.

ë‘ë²ˆì§¸ë¡œ ë‹¤ìŒì˜ pip installì„ í†µí•´ì„œ grpcë¥¼ ë‹¤ì‹œ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ì—ˆë‹¤.

`pip install --no-binary :all: grpcio --ignore-installed`

í•´ê²° ì•ˆë¬ë‹¤..

ë§ˆì§€ë§‰ìœ¼ë¡œ condaì˜ grpcë¥¼ ì´ìš©í•´ì„œ ì„¤ì¹˜í•˜ëŠ” ë°©ë²•ì„ ì°¾ì•„ì„œ ì§„í–‰í–ˆë‹¤

`conda install -c conda-forge grpcio`

ìœ„ì˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ì„œ ë‹¤ì‹œ ì„¤ì¹˜í–ˆë”ë‹ˆ ì§„í–‰ì´ ë¬ë‹¤ !~~ ğŸ‘

### ì˜ˆì œ ì½”ë“œ

```python
# bentoml-service.py

import bentoml
from bentoml.adapters import JsonInput
from bentoml.frameworks.transformers import TransformersModelArtifact
import numpy as np
from transformers import AutoModelForSequenceClassification, AutoTokenizer

@bentoml.env(pip_packages=["transformers==4.11.0", "torch==1.7.0"])
@bentoml.artifacts([TransformersModelArtifact("transformer")])
class TransformerService(bentoml.BentoService):

    LABELS = {
        0: "ë¶€ì •",
        1: "ê¸ì •"
    }

    @bentoml.api(input=JsonInput(), batch=False)
    def predict(self, parsed_json):
        src_text = parsed_json.get("text")

        model = self.artifacts.transformer.get("model")
        tokenizer = self.artifacts.transformer.get("tokenizer")

        # Pre-processing
        input_ids = tokenizer.encode(src_text, return_tensors="pt")

        # Model forward
        output = model(input_ids)

        # Post-processing
        preds = output.logits.detach().cpu().numpy()
        preds = np.argmax(preds, axis=1)

        return {
            "result": self.LABELS[preds[0]]
        }

if __name__ == "__main__":
    ts = TransformerService()

    model_name = "monologg/koelectra-small-finetuned-nsmc"
    model = AutoModelForSequenceClassification.from_pretrained("monologg/koelectra-small-finetuned-nsmc")
    tokenizer = AutoTokenizer.from_pretrained("monologg/koelectra-small-finetuned-nsmc")
    artifact = {"model": model, "tokenizer": tokenizer}

    ts.pack("transformer", artifact)
    saved_path = ts.save()
```

ìœ„ì˜ ì˜ˆì œ ì½”ë“œëŠ” ì˜í™” ë¦¬ë·°ë¥¼ ì…ë ¥ë°›ì•„ ê²°ê³¼ë¥¼ ë‚´ë„ë¡ pre-train ë˜ì–´ìˆëŠ” ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ì¸í¼ëŸ°ìŠ¤ ì„œë²„ë¡œ êµ¬ì„±í•˜ëŠ” ì˜ˆì œì´ë‹¤.

íŒŒì´ì¬ íŒŒì¼ì„ ì‘ì„±í•œ í›„ bentoml bundleì„ ë§Œë“¤ê³  dockerizeë¥¼ í•´ì„œ ë„ì»¤ì—ì„œ ì‹¤í–‰í•´ë³´ì•˜ë‹¤.

```python
python bentoml_service.py  # bentoml bundle ë§Œë“¤ê¸°
bentoml containerize TransformerService:latest -t tf_service:v1  # dockerize

docker run -p 5001:5000 tf_service:v1
```

ì²˜ìŒì— 5000ë²ˆ í¬íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤í–ˆìœ¼ë‚˜ ìê¾¸ ëˆ„ê°€ ì‚¬ìš©ì¤‘ì´ë¼ í™•ì¸í•´ë³´ë‹ˆ ControlCEë¼ëŠ” jobì´ ì‹¤í–‰ì¤‘ì´ì–´ì„œ 5001ë²ˆìœ¼ë¡œ ì§„í–‰í–ˆë‹¤. (ë‚˜ì¤‘ì— ì•Œê³ ë³´ë‹ˆ mac airplayë€ë‹¤â€¦)

ì‹¤í–‰í•´ì„œ ì ‘ê·¼í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ apië¥¼ ì„¤ëª…í•˜ëŠ” ì‚¬ì´íŠ¸ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled%202.png)

predictë¥¼ ìœ„í•´ì„œëŠ” http://0.0.0.0:5001/predict ë¥¼ POSTë¡œ inputì„ jsonìœ¼ë¡œ ë‹´ì•„ì„œ ë³´ë‚´ë©´ ëœë‹¤.

ìœ„ì˜ ì›¹í˜ì´ì§€ì—ì„œë„ ë³´ë‚´ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ”ë° appì˜ predictì—ì„œ ê°€ëŠ¥í•˜ë‹¤

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled%203.png)

Try it out ë²„íŠ¼ì„ ëˆ„ë¥´ê³ 

ìœ„ì— ì˜ˆì œ ì½”ë“œì—ì„œ json ë°ì´í„° ì¤‘ì— text í‚¤ë¥¼ íŒŒì‹±í•´ì„œ predict í•˜ë„ë¡ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ë°ì´í„°ë¥¼ ë„£ê³  execute ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ api ë¥¼ ìœë‹¤

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled%204.png)

ê·¸ëŸ¬ë©´ ì•„ë˜ì™€ ê°™ì€ ì‘ë‹µì´ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤.

![Untitled](../../assets/images/AI/MLOps/bentoml%20inference%20server/Untitled%205.png)

`ë„ˆë¬´ ë³„ë¡œë‹¤...` ë¼ëŠ” ë°ì´í„°ë¥¼ ì¶”ë¡ í•´ë³¸ ê²°ê³¼ `ë¶€ì •` ì´ ë„ì¶œëœê±¸ë³´ì•„ ì˜ ë˜ì—ˆë‹¤.